# -*- coding: utf-8 -*-
"""AI_RAG_Assignment_Tanishq_verma.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KUNnrWqM_gtLGYUlogHEBtYJ3horOe9M

## Local RAG Assistant (Tanishq Verma)

## SentenceTransformer (for embeddings)
# - FAISS (for vector storage)
# - DistilBERT (for question answering)
# - Streamlit (for UI)
"""

import streamlit as st
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import numpy as np
import faiss
from PyPDF2 import PdfReader
import docx
import pandas as pd
import csv
import re
import io

"""## Streamlit Setup"""

st.set_page_config(page_title="Local RAG Assistant", page_icon="üß†", layout="wide")

st.title("üß† Local RAG Assistant")
st.caption("Ask questions based only on your uploaded documents.")

"""## File Reader"""

def read_file(file):
    ext = file.name.split(".")[-1].lower()
    text = ""

    if ext == "txt":
        text = file.read().decode("utf-8", errors="ignore")

    elif ext == "pdf":
        reader = PdfReader(file)
        text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])

    elif ext == "docx":
        doc = docx.Document(file)
        text = "\n".join([para.text for para in doc.paragraphs])

    elif ext == "csv":
        df = pd.read_csv(file)
        text = df.to_string()

    else:
        st.warning(f"‚ö†Ô∏è Unsupported file type: {file.name}")
    return text

"""## Chunking

"""

def make_chunks(text, chunk_size=400, overlap=50):
    text = re.sub(r"\s+", " ", text.strip())
    sentences = re.split(r"(?<=[.!?]) +", text)

    chunks = []
    current_chunk = []

    for sentence in sentences:
        words = sentence.split()
        if len(current_chunk) + len(words) > chunk_size:
            chunks.append(" ".join(current_chunk))
            current_chunk = current_chunk[-overlap:] + words
        else:
            current_chunk += words

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks

"""## Initialize Global Objects"""

@st.cache_resource
def init_models():
    embed_model = SentenceTransformer("all-MiniLM-L6-v2")
    qa_model = pipeline("question-answering", model="distilbert-base-cased-distilled-squad")
    index = faiss.IndexFlatL2(embed_model.get_sentence_embedding_dimension())
    return embed_model, qa_model, index

embed_model, qa_model, index = init_models()
documents = []

"""## Upload Files"""

uploaded_files = st.file_uploader(
    "üìÇ Upload Documents (PDF, TXT, DOCX, CSV)",
    type=["pdf", "txt", "docx", "csv"],
    accept_multiple_files=True
)

if uploaded_files:
    total_chunks = 0
    for file in uploaded_files:
        text = read_file(file)
        chunks = make_chunks(text)
        if chunks:
            embeddings = embed_model.encode(chunks)
            index.add(np.array(embeddings, dtype="float32"))
            documents.extend(chunks)
            total_chunks += len(chunks)
    st.success(f"‚úÖ Uploaded and indexed {len(uploaded_files)} file(s) ({total_chunks} chunks).")

"""## Q&A Function"""

def answer_question(question):
    if len(documents) == 0 or index.ntotal == 0:
        return "‚ö†Ô∏è No documents available. Please upload files first."

    query_vector = embed_model.encode([question])
    D, I = index.search(np.array(query_vector, dtype="float32"), k=5)
    retrieved = [documents[i] for i in I[0] if 0 <= i < len(documents)]

    if not retrieved:
        return "ü§î I don't have enough information from the uploaded documents."

    answers = []
    for chunk in retrieved:
        try:
            res = qa_model(question=question, context=chunk)
            if res["answer"].strip() and res["score"] > 0.2:
                answers.append((res["answer"].strip(), res["score"]))
        except Exception:
            continue

    if not answers:
        return "ü§î I couldn't find any relevant answer in the uploaded documents."

    answers = sorted(answers, key=lambda x: x[1], reverse=True)
    top_answers = [a[0] for a in answers[:3]]
    avg_conf = np.mean([a[1] for a in answers[:3]])

    return "‚Ä¢ " + "\n‚Ä¢ ".join(top_answers) + f"\n\nüìä Confidence: {avg_conf:.2f}"

"""## Chat Interface"""

st.divider()
st.subheader("üí¨ Ask Your Question")

question = st.text_input("Type your question here:")

if st.button("üîç Get Answer"):
    if question.strip():
        with st.spinner("ü§î Searching your documents..."):
            response = answer_question(question)
        st.markdown(f"### üí° Answer:\n{response}")
    else:
        st.warning("‚ö†Ô∏è Please enter a question first.")

"""# REset Button

"""

st.divider()
col1, col2 = st.columns(2)

if col1.button("üßπ Clear Session"):
    st.cache_resource.clear()
    st.experimental_rerun()

if col2.button("üóëÔ∏è Reset Database"):
    index.reset()
    documents.clear()
    st.success("üóëÔ∏è All stored data cleared successfully.")

st.caption("---\nMade with ‚ù§Ô∏è by **Tanishq Verma** for Developer Bazaar Technologies")

